{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports:\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "gqG6Ag0T3clW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Training Data Generation"
      ],
      "metadata": {
        "id": "fdtmH3CakhN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumptions / constants (inputs)\n",
        "NUM_VERTICES = 5\n",
        "WEIGHT_MEAN = 9.0\n",
        "WEIGHT_STD = 5.0\n",
        "SOURCE_NODE_INDEX = 0 # Can also make random\n",
        "\n",
        "# Output: Adjacency matrix for the graph, and one hot encoded vectors for the source and sink node"
      ],
      "metadata": {
        "id": "5q8O21Wyun5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### EDGE WEIGHT GENERATORS ###\n",
        "\n",
        "def gaussian_edge_weight(edge_weight_mean = WEIGHT_MEAN,\n",
        "                         edge_weight_stddev = WEIGHT_STD,\n",
        "                         no_edge_prob = 0.2,\n",
        "                         whole_num_only = True):\n",
        "  \"\"\"\n",
        "  Generates a single edge weight that follows a normal distribution with the given parameters but negative weights are clipped to 0.\n",
        "  There is a no_edge_prob that the weight will instead get set to np.inf (to simulate that edge not existing in the graph)\n",
        "  \"\"\"\n",
        "  if random.random() < no_edge_prob:\n",
        "    return np.inf\n",
        "  else:\n",
        "    weight = max(0, np.random.normal(edge_weight_mean, edge_weight_stddev))\n",
        "    if whole_num_only: return np.floor(weight)\n",
        "    else: return weight\n",
        "\n",
        "def uniform_edge_weight(edge_weight_min = 0.0,\n",
        "                        edge_weight_max = 10.0,\n",
        "                        np_edge_prob = 0.2,\n",
        "                        whole_num_only = True):\n",
        "  \"\"\"\n",
        "  Generates a single edge weight that follows a uniform distribution with the given parameters.\n",
        "  There is a np_edge_prob that the weight will instead get set to np.inf (to simulate that edge not existing in the graph)\n",
        "  \"\"\"\n",
        "  if random.random() < np_edge_prob:\n",
        "    return np.inf\n",
        "  else:\n",
        "    weight = np.random.uniform(edge_weight_min, edge_weight_max)\n",
        "    if whole_num_only: return np.floor(weight)\n",
        "    else: return weight\n",
        "\n",
        "### END EDGE WEIGHT GENERATORS ###"
      ],
      "metadata": {
        "id": "2YfhCBUbuuZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOp-0-syyWjf"
      },
      "outputs": [],
      "source": [
        "def generate_shortest_path_problem(num_vertices,\n",
        "                                   source_index = SOURCE_NODE_INDEX,\n",
        "                                   edge_weight_generator = gaussian_edge_weight,\n",
        "                                   undirected = True):\n",
        "  \"\"\"\n",
        "  Generates a single shortest path problem with the specified number of vertices.\n",
        "  Edge weights are generated by edge_weight_generator (passed in function).\n",
        "\n",
        "  Output format:\n",
        "    - A, an (n+1) x n numpy matrix.\n",
        "    - The first row is a one hot encoded vector of the source node. (A[0][i] == 1 if i is the source node else 0)\n",
        "    - Each subsequent row is a row of the adjacency matrix.\n",
        "    - So A[i+1][j] represents the cost of the edge from i -> j. Therefore it is a constant that A[i+1][i] = 0 for all i.\n",
        "    - If undirected is True, then A[i+1][j] = A[j+1][i] for all i, j. Otherwise, they can be different\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate random graph:\n",
        "  A = np.zeros((num_vertices + 1, num_vertices))\n",
        "  A[0][source_index] = 1\n",
        "  if undirected:\n",
        "    for i in range(num_vertices):\n",
        "      for j in range(i+1, num_vertices):\n",
        "          A[i+1][j] = edge_weight_generator()\n",
        "          A[j+1][i] = A[i+1][j]\n",
        "  else:\n",
        "    for i in range(num_vertices):\n",
        "      for j in range(num_vertices):\n",
        "        if i != j:\n",
        "          A[i+1][j] = edge_weight_generator()\n",
        "\n",
        "  return A\n",
        "\n",
        "\n",
        "def solve_shortest_path_problem(A):\n",
        "  \"\"\"\n",
        "  Solves a shortest path problem that is in the format outputted by generate_shortest_path_problem.\n",
        "\n",
        "  Output format:\n",
        "    - S, a numpy array of length n. Each element is the length of the shortest path from the source node to that vertex.\n",
        "    - So S[i] = len of shortest path from source_index -> i. Therefore it is a constant that S[source_index] = 0.\n",
        "  \"\"\"\n",
        "\n",
        "  n = A.shape[1]\n",
        "  source_index = np.nonzero(A[0])[0][0]\n",
        "\n",
        "  # Solve graph with bellman ford (so we can add negative edges if desired):\n",
        "  S = np.full(n, np.inf)\n",
        "  S[source_index] = 0\n",
        "  for _ in range(n):\n",
        "    for i in range(n):\n",
        "      for j in range(n):\n",
        "        if S[i] + A[i+1][j] < S[j]:\n",
        "          S[j] = S[i] + A[i+1][j]\n",
        "\n",
        "  # Return\n",
        "  return S\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING CODE\n",
        "A = generate_shortest_path_problem(NUM_VERTICES, edge_weight_generator=gaussian_edge_weight, source_index=random.randint(0, NUM_VERTICES - 1))\n",
        "print(A)\n",
        "S = solve_shortest_path_problem(A)\n",
        "print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlh-Zn-VycGS",
        "outputId": "d256e598-1dd7-4d37-cd6e-14534f3bcc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  0.  0.  1.]\n",
            " [ 0. 14.  0.  7. 10.]\n",
            " [14.  0. 12.  2. inf]\n",
            " [ 0. 12.  0.  4.  9.]\n",
            " [ 7.  2.  4.  0. inf]\n",
            " [10. inf  9. inf  0.]]\n",
            "[ 9. 15.  9. 13.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(batch_size,\n",
        "                   num_vertices = NUM_VERTICES,\n",
        "                   source_index = SOURCE_NODE_INDEX,\n",
        "                   edge_weight_generator = gaussian_edge_weight,\n",
        "                   undirected = True):\n",
        "  \"\"\"\n",
        "  Generates a batch of shortest path problems. Outputs two np tensors, A_batch and S_batch\n",
        "\n",
        "  All problems in the batch have to have the same \"format\" (num_vertices, source_index, edge_weight_generator, undirected)\n",
        "\n",
        "  A_batch: np array of shape (batch_size, num_vertices + 1, num_vertices)\n",
        "  S_batch: np array of shape (batch_size, num_vertices)\n",
        "  \"\"\"\n",
        "\n",
        "  A_list = []\n",
        "  S_list = []\n",
        "  for _ in range(batch_size):\n",
        "    A = generate_shortest_path_problem(num_vertices, source_index, edge_weight_generator, undirected)\n",
        "    S = solve_shortest_path_problem(A)\n",
        "    A_list.append(A)\n",
        "    S_list.append(S)\n",
        "\n",
        "  A_batch = np.stack(A_list, axis=0)\n",
        "  S_batch = np.stack(S_list, axis=0)\n",
        "\n",
        "  return A_batch, S_batch"
      ],
      "metadata": {
        "id": "STWumRZyuysS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transformer Architecture Setup"
      ],
      "metadata": {
        "id": "CaRbeQOru0QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Transformer Architecture Setup][-olk,m ]\n",
        "# Input: Graph as an adjacency matrix inputted as vectors with one hot encoding vectors added for the source and sink node\n",
        "# --> Tokens are the vectors in adjacency matrix, one for each vertex\n",
        "# Output: Option 1: Real valued output for length of shortest path; Option 2: reconstructed path\n",
        "#   Option 3: Could also output a vector which for V[i] = length of shortest path to vertex i (single source shortest paths)\n",
        "#   in this case we don't need one hot encoded source/sink vectors as input\n",
        "# Loss: Opt1: Squared error; Opt2: Frobenius norm"
      ],
      "metadata": {
        "id": "BySsEn72znxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.0):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, model_dim)  # Project input to model dimension\n",
        "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=2*model_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=8*model_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_proj = nn.Linear(2*model_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: (batch_size, sequence_length, input_dim)\n",
        "        src = self.input_proj(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        x = self.transformer_encoder(src)\n",
        "        output = self.output_proj(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dim, dropout=0.0, max_len=500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, model_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, model_dim)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, model_dim)\n",
        "        # x = x + self.pe[:, :x.size(1)]\n",
        "        x = torch.stack((x, self.pe[:, :x.size(1)]), dim=1)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "_hjKrNk-lrI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Transformer Training"
      ],
      "metadata": {
        "id": "h6dy4kwM8pE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Training Time Transformer\n",
        "# Input: ___ number of randomly generated examples from the training data generation and the transformer model\n",
        "# Output: None\n",
        "\n",
        "batch_size = 32\n",
        "num_vertices = 5\n",
        "input_dim = 5\n",
        "output_dim = 5\n",
        "\n",
        "model_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "\n",
        "num_batches = 10_000\n",
        "\n",
        "model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "losses = [] # loss per batch\n",
        "model.train()\n",
        "for i in range(num_batches):\n",
        "  x_batch, y_batch = generate_batch(batch_size, source_index = i % num_vertices, num_vertices=num_vertices, edge_weight_generator=gaussian_edge_weight)\n",
        "  x_batch = torch.tensor(x_batch, dtype=torch.float32).to(device)\n",
        "  y_batch = torch.tensor(y_batch, dtype=torch.float32).to(device)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  output = model(x_batch)\n",
        "  loss = loss_fn(output, y_batch)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "  if i % 100 == 0:\n",
        "    print(f\"Batch {i+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SpEse4f1zrM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transformer Testing"
      ],
      "metadata": {
        "id": "uXPY_6hO8roH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Test Time Transformer\n",
        "# Input: Newly generated random examples, can experiment with different number of vertices to check ICL capability\n",
        "# Output: Accuracy measurements (figure out what these should be)"
      ],
      "metadata": {
        "id": "m5EuTduxzvEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. (Optional) Project Extensions"
      ],
      "metadata": {
        "id": "0WtvyYC88ymN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Extensions of Project (Optional)\n",
        "# Choices:\n",
        "# Add higher number of vertices to make the graph more complex\n",
        "# Add the option 2 for output of model - output entire path instead of just length\n",
        "# Make the edge weights non-negative or even negative (adjust training data to use bellman-ford)\n",
        "# Add A* search capability with admissible heuristics into the training data"
      ],
      "metadata": {
        "id": "oPrG8XfB2JMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}